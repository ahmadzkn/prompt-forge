{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ‚ö° PromptForge: Local Colab Edition\n",
                "\n",
                "Run the full PromptForge optimization pipeline **locally** on this Google Colab instance using a GPU-accelerated Llama.cpp backend.\n",
                "\n",
                "**No API Keys required. No Cloud data transfer. 100% Private.**\n",
                "\n",
                "### Selected Model:\n",
                "By default, we will download and use **Phi-3 Mini (3.8B)** (Quantized) which fits perfectly on the free T4 GPU and provides excellent instruction adherence."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 1. Setup Environment (Fast Install)\n",
                "# Check GPU\n",
                "!nvidia-smi\n",
                "\n",
                "# Install llama-cpp-python using pre-built CUDA wheels (Much faster than compiling)\n",
                "!pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121\n",
                "\n",
                "# Install other dependencies\n",
                "!pip install sqlalchemy pydantic ipywidgets huggingface_hub ollama openai anthropic google-generativeai groq\n",
                "\n",
                "# Clone App\n",
                "!git clone https://github.com/ahmadzkn/prompt-forge.git\n",
                "%cd prompt-forge\n",
                "\n",
                "# Fix for Colab environment import paths\n",
                "import sys\n",
                "import os\n",
                "sys.path.append(os.getcwd())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 2. Download Model (Phi-3 Mini Instruct)\n",
                "from huggingface_hub import hf_hub_download\n",
                "\n",
                "model_repo = \"microsoft/Phi-3-mini-4k-instruct-gguf\"\n",
                "model_filename = \"Phi-3-mini-4k-instruct-q4.gguf\"\n",
                "\n",
                "print(f\"Downloading {model_filename} from {model_repo}...\")\n",
                "try:\n",
                "    model_path = hf_hub_download(repo_id=model_repo, filename=model_filename)\n",
                "    print(f\"\\n‚úÖ Model downloaded to: {model_path}\")\n",
                "except Exception as e:\n",
                "    print(f\"Download Error: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 3. Run Optimizer\n",
                "import ipywidgets as widgets\n",
                "from IPython.display import display, clear_output\n",
                "from src.optimizer import PromptOptimizer\n",
                "import traceback\n",
                "\n",
                "# Initialize with LlamaCpp\n",
                "print(\"üöÄ Loading Model into GPU memory... (This takes a few seconds)\")\n",
                "\n",
                "try:\n",
                "    optimizer = PromptOptimizer(\n",
                "        provider_type=\"llamacpp\",\n",
                "        model_path=model_path,\n",
                "        n_gpu_layers=-1, # Offload all to GPU\n",
                "        n_ctx=4096\n",
                "    )\n",
                "    print(\"‚úÖ Model Loaded Successfully!\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Error loading model: {e}\")\n",
                "    traceback.print_exc()\n",
                "\n",
                "# UI\n",
                "raw_prompt_area = widgets.Textarea(\n",
                "    placeholder='Enter your raw prompt here...',\n",
                "    description='Raw Prompt:',\n",
                "    layout=widgets.Layout(width='100%', height='150px')\n",
                ")\n",
                "\n",
                "optimize_btn = widgets.Button(\n",
                "    description='Optimize Prompt',\n",
                "    button_style='primary',\n",
                "    icon='magic'\n",
                ")\n",
                "\n",
                "output_area = widgets.Output()\n",
                "\n",
                "def on_optimize(b):\n",
                "    with output_area:\n",
                "        clear_output()\n",
                "        print(\"Optimizing... Please wait.\")\n",
                "        try:\n",
                "            # 'local' model name is ignored by LlamaCpp backend but required by interface\n",
                "            result = optimizer.optimize_prompt(raw_prompt_area.value, model=\"local\")\n",
                "            \n",
                "            if \"error\" in result:\n",
                "                print(f\"‚ùå Error: {result['error']}\")\n",
                "                return\n",
                "\n",
                "            print(\"### Final Optimized Prompt\")\n",
                "            print(\"-\" * 80)\n",
                "            print(result.get(\"final_prompt\", \"No result\"))\n",
                "            print(\"-\" * 80)\n",
                "            \n",
                "            print(\"\\n### Structured Elements:\")\n",
                "            elements = result.get(\"elements\", {})\n",
                "            for k, v in elements.items():\n",
                "                print(f\"**{k.title()}**: {v}\")\n",
                "        except Exception as e:\n",
                "            print(f\"‚ùå Execution Error: {e}\")\n",
                "            traceback.print_exc()\n",
                "\n",
                "optimize_btn.on_click(on_optimize)\n",
                "\n",
                "display(widgets.VBox([\n",
                "    widgets.HTML(\"<h2>PromptForge Local Optimizer</h2>\"),\n",
                "    widgets.HTML(\"<p>Enter a raw idea and let the local Phi-3 model optimize it for you.</p>\"),\n",
                "    raw_prompt_area,\n",
                "    optimize_btn,\n",
                "    output_area\n",
                "]))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}